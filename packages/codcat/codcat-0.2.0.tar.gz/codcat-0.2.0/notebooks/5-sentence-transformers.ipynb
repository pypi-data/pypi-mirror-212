{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaabc3eb-0206-4b52-85bb-824fd6de1c67",
   "metadata": {},
   "source": [
    "# Sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44099964-f3c0-44c0-b1fe-6bca03db0d82",
   "metadata": {},
   "source": [
    "Sentence-transformers are a type of natural language processing (NLP) model that is trained to convert a given sentence or text into a vector representation that captures the semantic meaning of the text. These vector representations can then be used for a variety of tasks such as text classification, semantic similarity, and text clustering, among others.\n",
    "\n",
    "`microsoft/codebert-base` is a pre-trained language model that was specifically trained on source code tasks. It is a variant of the original BERT model, which was pre-trained on a large corpus of text data. The CodeBERT-base model was pre-trained on a massive dataset of 6 million lines of code, which was sourced from various open-source software projects.\n",
    "\n",
    "The reason for using Microsoft/CodeBERT-base as the base model for sentence-transformers is that it has several advantages over other pre-trained language models. \n",
    "\n",
    "Firstly, as it was trained on source code tasks, it has a better understanding of the syntactical and structural components of code, which makes it more suitable for tasks related to code analysis and code generation. This is particularly useful for developers who are looking to automate certain parts of their coding process.\n",
    "\n",
    "Secondly, `microsoft/codebert-base` has been fine-tuned on a variety of programming-related tasks, such as code completion, code summarization, and code search. This means that it has a better understanding of the programming domain and can provide more accurate and relevant results for programming-related tasks.\n",
    "\n",
    "Finally, `microsoft/codebert-base` has been shown to perform well on a variety of downstream NLP tasks, such as sentence classification and semantic similarity. This is due to its ability to capture the semantic meaning of the text, which is essential for many NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b82254c-8acc-4937-93c2-5c0042240e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca29599-d04b-4772-8804-4342ad681ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/qnbhd/.cache/torch/sentence_transformers/microsoft_codebert-base. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('microsoft/codebert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1593c3f4-92af-4ef5-831e-a9f3675b3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f43382-b790-4cb6-8c56-dfa4adbf6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train-preprocessed.csv')\n",
    "test = pd.read_csv('test-preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb0dfe98-aa71-44c0-a9ec-884eb3ed599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = model.encode(train['code'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e6611e-f097-4c48-a109-5f14a3e6cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_embeddings = model.encode(test['code'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7feb223-5c8e-4e48-abc3-370580f1f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "742fcb99-46f2-4e6c-9427-190cd42e4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LGBMClassifier(random_state=42).fit(X_train_embeddings, train['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "376c0174-6b89-4a61-827a-e006d2edd9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd0148dc-e3f0-46c3-9ff3-04290d9ee02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           c       0.86      0.93      0.89       229\n",
      "         cpp       0.88      0.82      0.85       230\n",
      "         css       0.96      0.98      0.97       220\n",
      "     haskell       0.89      0.91      0.90       221\n",
      "        html       0.94      0.95      0.95       240\n",
      "        java       0.94      0.99      0.96       216\n",
      "  javascript       0.95      0.95      0.95       219\n",
      "         lua       0.90      0.89      0.89       218\n",
      "        objc       0.97      0.94      0.95       243\n",
      "        perl       0.96      0.95      0.95       225\n",
      "         php       0.97      0.94      0.95       237\n",
      "      python       0.96      0.94      0.95       225\n",
      "           r       0.91      0.94      0.92       214\n",
      "        ruby       0.91      0.92      0.92       208\n",
      "       scala       0.89      0.90      0.89       198\n",
      "      sqlite       0.95      0.93      0.94       209\n",
      "       swift       0.97      0.91      0.94       210\n",
      "\n",
      "    accuracy                           0.93      3762\n",
      "   macro avg       0.93      0.93      0.93      3762\n",
      "weighted avg       0.93      0.93      0.93      3762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['language'], clf.predict(X_test_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda06c5-b1a2-4cb2-b72b-c9b6f48dc3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
