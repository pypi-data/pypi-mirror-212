# -*- coding: utf-8 -*-
"""DocumentInsightsGenerator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15QKnS-xYgGVFtQDuQaDIZg8VndtIX-mr
"""

import spacy
import requests
from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pdfplumber
import pytesseract
from pdfminer.converter import PDFPageAggregator
from pdfminer.layout import LAParams, LTFigure
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from PIL import Image
from pdfminer.layout import LTImage
import io
import docx2txt


class DocumentInsightsGenerator:
    def __init__(self, api_key):
        self.doc_text = ""
        self.insights = ""
        self.api_key = api_key

        # Load the pre-trained GPT model and tokenizer
        self.model = GPT2LMHeadModel.from_pretrained('gpt2')
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

        # Load NER model for named entity recognition
        self.ner_pipeline = pipeline('ner', model='dslim/bert-base-NER')

        # Load spaCy model for heading detection
        self.nlp = spacy.load("en_core_web_sm")

    def load_document(self, file_path):
        if file_path.endswith('.pdf'):
            self.load_pdf(file_path)
        elif file_path.endswith('.docx'):
            self.load_word_doc(file_path)
        else:
            raise ValueError("Unsupported file format. Only PDF and DOCX files are supported.")

    def load_pdf(self, file_path):
        # Extract text from the pages of the PDF
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                self.doc_text += page.extract_text()

    def load_word_doc(self, file_path):
        # Extract text from the Word document
        self.doc_text = docx2txt.process(file_path)

    def generate_insights(self):
        # Split the document text into chunks
        chunk_size = 3000  # Set the desired chunk size
        chunks = [self.doc_text[i:i + chunk_size] for i in range(0, len(self.doc_text), chunk_size)]

        # Apply NLP techniques to extract insights from each chunk
        for chunk in chunks:
            # 1. Keyword Extraction using TF-IDF
            vectorizer = TfidfVectorizer()
            X = vectorizer.fit_transform([chunk])
            feature_names = vectorizer.get_feature_names_out()
            top_keywords = sorted(zip(feature_names, X.sum(axis=0).tolist()[0]), key=lambda x: x[1], reverse=True)[:10]

            self.insights += "Keywords:\n"
            for keyword, score in top_keywords:
                self.insights += f"{keyword}: {score}\n"

            # 2. Named Entity Recognition (NER)
            entities = self.ner_pipeline(chunk)
            self.insights += "Named Entities:\n"
            for entity in entities:
                self.insights += f"Text: {entity['word']}, Label: {entity['entity']}\n"

            # 3. Topic Modeling
            # Vectorize the text using TfidfVectorizer
            vectorizer = TfidfVectorizer()
            X = vectorizer.fit_transform([chunk])

            # Perform Latent Dirichlet Allocation for topic modeling
            num_topics = 5  # Number of topics to identify
            lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
            lda.fit(X)

            self.insights += "Topics:\n"
            for topic_idx, topic in enumerate(lda.components_):
                top_words_idx = topic.argsort()[:-5 - 1:-1]
                top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]
                self.insights += f"Topic {topic_idx + 1}: {', '.join(top_words)}\n"

    def answer_question(self, question):
            # Prepare the payload for the API request
            payload = {
                'prompt': self.doc_text + "\n" + question,
                'max_tokens': 100,
                'temperature': 0.5,
                'top_p': 1.0,
                'frequency_penalty': 0.0,
                'presence_penalty': 0.0
            }

            # Make the API request
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            response = requests.post('https://api.openai.com/v1/engines/text-davinci-003/completions', json=payload,
                                    headers=headers)

            # Check the status code of the response
            if response.status_code != 200:
                print(f"Error in API request: {response.json()['error']['message']}")
                return None

            # Process the API response and extract the generated response
            data = response.json()
            generated_response = data['choices'][0]['text'].strip()

            # Verify if the information in the answer is in the document
            for sentence in generated_response.split('.'):
                if sentence.strip() and sentence.strip().lower() not in self.doc_text.lower():
                    return "The document does not contain information on your question."
            
            return generated_response
     
    def find_reference(self, prompt, generated_response):
        # Search for the prompt in the document text and get the page number
        page_number = self.find_page_number(prompt)

        if page_number is not None:
            # Find the nearest heading on the page
            heading = self.find_nearest_heading(page_number)

            # Construct the reference
            if heading:
                return f"Page {page_number}, Section: {heading}"
            else:
                return f"Page {page_number}"

        return None

    def find_page_number(self, prompt):
        # Search for the prompt in the document text and get the page number
        pages = self.doc_text.split("\n\n\n")  # Split the document text into pages
        for i, page in enumerate(pages, start=1):
            if prompt in page:
                return i

        return None

    def find_nearest_heading(self, page_number):
        # Find the nearest heading on the page
        page_text = self.get_page_text(page_number)
        headings = []

        for element in page_text:
            doc = self.nlp(element['text'])
            for ent in doc.ents:
                if ent.label_ == 'ORG' or ent.label_ == 'PERSON':
                    headings.append(element['text'])
                    break

        if headings:
            # Return the heading with the highest confidence
            return headings[0]

        return None

    def get_page_text(self, page_number):
        # Extract the text of a specific page
        with pdfplumber.open(io.BytesIO(self.doc_text)) as pdf:
            page = pdf.pages[page_number - 1]
            return page.extract_words(x_tolerance=2, y_tolerance=2)

    def extract_document_info(self):
        # Extract document information such as title, author, purpose, key points, summary, and references
        title = self.find_nearest_heading(1)  # Assuming title is usually found on the first page
        author = self.find_nearest_heading(1)  # Assuming author information is often near the title
        purpose = self.find_nearest_heading(2)  # Assuming the purpose is usually described early in the document
        key_points = self.find_key_points()  # Custom method to extract key points
        summary = self.generate_summary()  # Custom method to generate a summary
        references = self.find_references()  # Custom method to extract reference information

        return {
            'Title': title,
            'Author': author,
            'Purpose': purpose,
            'Key Points': key_points,
            'Summary': summary,
            'References': references
        }
    
if __name__ == "__main__":
        # Prompt user to enter the API key
        api_key = input("Please enter your API key: ")

        # Initialize the DocumentInsightsGenerator with the API key
        dig = DocumentInsightsGenerator(api_key=api_key)

        # Prompt user to upload a document
        file_path = input("Please enter the path of the PDF you want to upload: ")

        # Load the uploaded document
        dig.load_document(file_path)

        # Let the user ask questions until they type 'QUIT'
        while True:
            question = input("Ask a question about the document (or type 'QUIT' to exit): ")
            if question.upper() == 'QUIT':
                break
            else:
                answer = dig.answer_question(question)
                print(f"Answer: {answer}\n")
