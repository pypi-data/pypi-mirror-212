Metadata-Version: 2.1
Name: bettertest
Version: 0.1.97
Summary: 
Author: krrishdholakia
Author-email: krrishdholakia@gmail.com
Requires-Python: >=3.8,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: openai (>=0.27.2,<0.28.0)
Requires-Dist: supabase (>=1.0.3,<2.0.0)
Requires-Dist: tenacity (>=8.2.2,<9.0.0)
Description-Content-Type: text/markdown

# bettertest 📝🔍

⚡ A Python testing library for automatically evaluating and tracing LLM applications ⚡

Our goal with bettertest is to simplify the process of testing and debugging LLM applications. It automatically evaluates your model's responses against your solution answers (auto-eval) and provides tracing features out-of-the-box.

With bettertest, you can automatically test your LLM applications and view print statements for each run just by adding 'bettertest' to any print statement in your code.

## Getting Started

Before using BetterTest, you need to install it via pip:

```
pip install bettertest
```

After installation, import the BetterTest library in your Python project:

```python
from bettertest import BetterTest
```
## Using BetterTest

### Initialize BetterTest

Create an instance of the BetterTest class with the user's email:

```python
bt = BetterTest("your_email@example.com", "your_openai_api_key")
```

Replace `"your_email@example.com"` with the appropriate email address.
Replace `"your_openai_api_key"` with your openai api key. [Here's where to find it](https://platform.openai.com/account/api-keys).

### Evaluate Model Responses

The `eval()` function takes in a list of questions, a list of answers, an LLM function, and an optional `num_runs` argument. It automatically evaluates the model's response against the solution answer and provides tracing for each run. Use it as follows:

```python
questions = [...]  # List of questions
answers = [...]    # List of corresponding solution answers

def llm_function(question):
    # Your custom LLM function implementation goes here
    pass

bt = BetterTest("your_email@example.com")
bt.eval(questions, answers, llm_function)
```

Replace the `llm_function` with your LLM function, and customize `num_runs` if necessary. By default, `num_runs` is set to 1.


## Contributing

We welcome contributions to InstructPrompt! Feel free to create issues/PR's/or DM us (👋 Hi I'm Krrish - +17708783106)

## Changelog

The current version of BetterTest is `0.1.9`.

## License

BetterTest is released under the [MIT License](https://github.com/bettertest/readme/blob/master/LICENSE).

