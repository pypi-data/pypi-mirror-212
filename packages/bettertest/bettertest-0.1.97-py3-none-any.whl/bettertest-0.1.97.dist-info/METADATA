Metadata-Version: 2.1
Name: bettertest
Version: 0.1.97
Summary: 
Author: krrishdholakia
Author-email: krrishdholakia@gmail.com
Requires-Python: >=3.8,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: openai (>=0.27.2,<0.28.0)
Requires-Dist: supabase (>=1.0.3,<2.0.0)
Requires-Dist: tenacity (>=8.2.2,<9.0.0)
Description-Content-Type: text/markdown

# bettertest ðŸ“ðŸ”

âš¡ A Python testing library for automatically evaluating and tracing LLM applications âš¡

Our goal with bettertest is to simplify the process of testing and debugging LLM applications. It automatically evaluates your model's responses against your solution answers (auto-eval) and provides tracing features out-of-the-box.

With bettertest, you can automatically test your LLM applications and view print statements for each run just by adding 'bettertest' to any print statement in your code.

## Getting Started

Before using BetterTest, you need to install it via pip:

```
pip install bettertest
```

After installation, import the BetterTest library in your Python project:

```python
from bettertest import BetterTest
```
## Using BetterTest

### Initialize BetterTest

Create an instance of the BetterTest class with the user's email:

```python
bt = BetterTest("your_email@example.com", "your_openai_api_key")
```

Replace `"your_email@example.com"` with the appropriate email address.
Replace `"your_openai_api_key"` with your openai api key. [Here's where to find it](https://platform.openai.com/account/api-keys).

### Evaluate Model Responses

The `eval()` function takes in a list of questions, a list of answers, an LLM function, and an optional `num_runs` argument. It automatically evaluates the model's response against the solution answer and provides tracing for each run. Use it as follows:

```python
questions = [...]  # List of questions
answers = [...]    # List of corresponding solution answers

def llm_function(question):
    # Your custom LLM function implementation goes here
    pass

bt = BetterTest("your_email@example.com")
bt.eval(questions, answers, llm_function)
```

Replace the `llm_function` with your LLM function, and customize `num_runs` if necessary. By default, `num_runs` is set to 1.


## Contributing

We welcome contributions to InstructPrompt! Feel free to create issues/PR's/or DM us (ðŸ‘‹ Hi I'm Krrish - +17708783106)

## Changelog

The current version of BetterTest is `0.1.9`.

## License

BetterTest is released under the [MIT License](https://github.com/bettertest/readme/blob/master/LICENSE).

